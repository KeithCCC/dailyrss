{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class rsss_feed:\n",
    "    def __init__(self, source_site_name, title, url, uri, created_date):\n",
    "        self.source_site_name = source_site_name\n",
    "        self.title = title\n",
    "        self.url = url\n",
    "        self.uri = uri\n",
    "        self.created_date = created_date\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"rsss_feed(title={self.title}, url={self.url})\"\n",
    "\n",
    "class source_site:\n",
    "    def __init__(self, title, url):\n",
    "        self.title = title\n",
    "        self.url = url\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"source_site(titel={self.title}, url={self.url})\"\n",
    "    \n",
    "    def add_rss_feed(self, rss_feed):\n",
    "        self.children.append(rss_feed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = source_site(\"source\", \"@IT\", \"https://atmarkit.itmedia.co.jp/\", \"https://atmarkit.itmedia.co.jp/\", \"Atmark IT\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@IT\n"
     ]
    }
   ],
   "source": [
    "print(source.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "initial_data = [[0,\"source\", \"@IT\", \"\\\"https://atmarkit.itmedia.co.jp/\\\"\", \"\\\"https://atmarkit.itmedia.co.jp/\\\"\", \"Atmark IT\"]]\n",
    "\n",
    "df = pd.DataFrame(initial_data, columns=[\"uid\", \"Type\", \"Title\", \"URL\", \"URI\", \"Description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>Type</th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>URI</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>source</td>\n",
       "      <td>@IT</td>\n",
       "      <td>\"https://atmarkit.itmedia.co.jp/\"</td>\n",
       "      <td>\"https://atmarkit.itmedia.co.jp/\"</td>\n",
       "      <td>Atmark IT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid    Type Title                                URL  \\\n",
       "0    0  source   @IT  \"https://atmarkit.itmedia.co.jp/\"   \n",
       "\n",
       "                                 URI Description  \n",
       "0  \"https://atmarkit.itmedia.co.jp/\"   Atmark IT  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringA = df[df['uid'] == 0]['URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[3,2,1,0]]\n",
    "\n",
    "df1 = pd.DataFrame(data1, columns=[\"A\", \"B\", \"C\", \"D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2\n",
      "Name: B, dtype: object\n"
     ]
    }
   ],
   "source": [
    "strA = df1[df1['A'] == 3]['B']\n",
    "print (strA.astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "strB = df1.query('A == 3')['B']\n",
    "elementB = df1.loc[0, 'B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(elementB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# initial_data = [[0,\"source\", \"@IT\", \"\\\"https://atmarkit.itmedia.co.jp/\\\"\", \"\\\"https://atmarkit.itmedia.co.jp/\\\"\", \"Atmark IT\"]]\n",
    "initial_data = [[0,\"source\", \"@IT\", \"https://atmarkit.itmedia.co.jp/\", \"https://atmarkit.itmedia.co.jp/\", \"Atmark IT\"]]\n",
    "  \n",
    "    \n",
    "sites_df = pd.DataFrame(initial_data, columns=[\"uid\", \"Type\", \"Title\", \"URL\", \"URI\", \"Description\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_site_url=\"https://akiba-pc.watch.impress.co.jp/\"\n",
    "new_row = pd.DataFrame({'URL': [new_site_url], 'URI': [new_site_url]})\n",
    "sites_df= pd.concat([sites_df, new_row], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def sample_df():\n",
    "    initial_data = [[0,\"source\", \"@IT\", \"https://atmarkit.itmedia.co.jp/\", \"https://atmarkit.itmedia.co.jp/\", \"Atmark IT\"]]\n",
    "    sample_df = pd.DataFrame(initial_data, columns=[\"uid\", \"Type\", \"Title\", \"URL\", \"URI\", \"Description\"])\n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"default.json\"\n",
    "if not os.path.exists(filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        sites_df.to_json(filename, orient='records')\n",
    "else:\n",
    "    sites_df = pd.read_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-25\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Date string\n",
    "date_str = \"Thu, 25 Jul 2024 10:00:00 +0900\"\n",
    "\n",
    "# Parse the date string\n",
    "date_object = datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S %z')\n",
    "\n",
    "# Print the resulting datetime object\n",
    "print(date_object.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "# String variables\n",
    "text = \"Hello, World!\"\n",
    "color = \"blue\"\n",
    "font_size = \"24px\"\n",
    "\n",
    "# Create the HTML string with the variables\n",
    "html_str = f\"<p style='font-size:{font_size}; color:{color};'>{text}</p>\"\n",
    "\n",
    "# Display the styled text in Streamlit\n",
    "st.markdown(html_str, unsafe_allow_html=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "website_url = \"https://atmarkit.itmedia.co.jp/\"\n",
    "\n",
    "response = requests.get(website_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "rss_urls = []\n",
    "for link in soup.find_all('link', type='application/rss+xml'):\n",
    "    rss_urls.append(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://rss.itmedia.co.jp/rss/0.91/ait.xml',\n",
       " 'https://rss.itmedia.co.jp/rss/1.0/ait.xml',\n",
       " 'https://rss.itmedia.co.jp/rss/2.0/ait.xml']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rss_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "url = \"https://news.yahoo.co.jp/rss/topics/top-picks.xml\"\n",
    "\n",
    "feed = feedparser.parse(url)\n",
    "for entry in feed.entries:\n",
    "    if 'published' in entry:\n",
    "        print(entry.published)\n",
    "    print(entry.title)\n",
    "    print(entry.link)\n",
    "    #print(entry.summary)\n",
    "    print('---------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from xml.etree import ElementTree\n",
    "import streamlit  as st\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "filename_news= \"newsfeed.json\"\n",
    "news_df = pd.read_json(filename_news)\n",
    "\n",
    "print(news_df)\n",
    "\n",
    "rss_feeds = [news_df.loc[1, 'URL']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://news.yahoo.co.jp/rss/topics/top-picks.xml']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rss_feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue, 13 Aug 2024 10:09:08 GMT\n",
      "東海道新幹線16-17日運休の可能性\n",
      "https://news.yahoo.co.jp/pickup/6510601?source=rss\n",
      "---------------------------------\n",
      "Tue, 13 Aug 2024 08:05:35 GMT\n",
      "全国的に来週も厳しい暑さ続く\n",
      "https://news.yahoo.co.jp/pickup/6510580?source=rss\n",
      "---------------------------------\n",
      "Tue, 13 Aug 2024 09:26:07 GMT\n",
      "4歳泣いて海へ 米軍に狙われた船\n",
      "https://news.yahoo.co.jp/pickup/6510595?source=rss\n",
      "---------------------------------\n",
      "Tue, 13 Aug 2024 11:22:41 GMT\n",
      "保育園が突然休園「ありえない」\n",
      "https://news.yahoo.co.jp/pickup/6510607?source=rss\n",
      "---------------------------------\n",
      "Tue, 13 Aug 2024 11:15:56 GMT\n",
      "「貯金は150万円」新富裕層の実態\n",
      "https://news.yahoo.co.jp/pickup/6510606?source=rss\n",
      "---------------------------------\n",
      "Tue, 13 Aug 2024 11:13:22 GMT\n",
      "高温の甲子園 球児相次ぎ足に異変\n",
      "https://news.yahoo.co.jp/pickup/6510609?source=rss\n",
      "---------------------------------\n",
      "Tue, 13 Aug 2024 10:17:53 GMT\n",
      "無課金おじさん「心の中では嵐」\n",
      "https://news.yahoo.co.jp/pickup/6510603?source=rss\n",
      "---------------------------------\n",
      "Tue, 13 Aug 2024 11:59:44 GMT\n",
      "梅宮アンナ、乳がんり患を報告\n",
      "https://news.yahoo.co.jp/pickup/6510610?source=rss\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "for item in rss_feeds:\n",
    "    feed = feedparser.parse(item)\n",
    "    # st.text(rss_feeds)\n",
    "    # st.text(len(feed))\n",
    "    # st.text(feed)\n",
    "    for entry in feed.entries:\n",
    "        if 'published' in entry:\n",
    "            print(entry.published)\n",
    "        print(entry.title)\n",
    "        print(entry.link)\n",
    "        print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add news rss from site\n",
      "   uid    Type                                              Title  \\\n",
      "0  0.0  source                                                @IT   \n",
      "1  NaN    None  https://news.yahoo.co.jp/rss/topics/top-picks.xml   \n",
      "\n",
      "                                                 URL  \\\n",
      "0                    https://atmarkit.itmedia.co.jp/   \n",
      "1  https://news.yahoo.co.jp/rss/topics/top-picks.xml   \n",
      "\n",
      "                                                 URI Description  \n",
      "0                    https://atmarkit.itmedia.co.jp/   Atmark IT  \n",
      "1  https://news.yahoo.co.jp/rss/topics/top-picks.xml              \n",
      "0    https://atmarkit.itmedia.co.jp/\n",
      "Name: URL, dtype: object\n",
      "Add news rss\n",
      "   uid  Type                                              Title  \\\n",
      "1  NaN  None  https://news.yahoo.co.jp/rss/topics/top-picks.xml   \n",
      "\n",
      "                                                 URL  \\\n",
      "1  https://news.yahoo.co.jp/rss/topics/top-picks.xml   \n",
      "\n",
      "                                                 URI Description  \n",
      "1  https://news.yahoo.co.jp/rss/topics/top-picks.xml              \n"
     ]
    }
   ],
   "source": [
    "from xml.etree import ElementTree\n",
    "import streamlit  as st\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "filename_news= \"newsfeed.json\"\n",
    "news_df = pd.read_json(filename_news)\n",
    "\n",
    "print(\"Add news rss from site\")\n",
    "\n",
    "print(news_df)\n",
    "# print(news_df.query('Type == source')['url'])\n",
    "print(news_df[news_df['Type'] == 'source']['URL'])\n",
    "\n",
    "print(\"Add news rss\")\n",
    "print(news_df[news_df['Type'] != 'source'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source @IT\n",
      "0\n",
      "None https://news.yahoo.co.jp/rss/topics/top-picks.xml\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "news_df\n",
    "df=news_df\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "        # Create a new row with columns\n",
    "        row_as_list = row.to_list()\n",
    "\n",
    "        # Display the item in the first column\n",
    "        print(row_as_list[1],row_as_list[2])\n",
    "        print(index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " None,\n",
       " 'https://news.yahoo.co.jp/rss/topics/top-picks.xml',\n",
       " 'https://news.yahoo.co.jp/rss/topics/top-picks.xml',\n",
       " 'https://news.yahoo.co.jp/rss/topics/top-picks.xml',\n",
       " '']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_as_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>Type</th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>URI</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>source</td>\n",
       "      <td>@IT</td>\n",
       "      <td>https://atmarkit.itmedia.co.jp/</td>\n",
       "      <td>https://atmarkit.itmedia.co.jp/</td>\n",
       "      <td>Atmark IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>https://news.yahoo.co.jp/rss/topics/top-picks.xml</td>\n",
       "      <td>https://news.yahoo.co.jp/rss/topics/top-picks.xml</td>\n",
       "      <td>https://news.yahoo.co.jp/rss/topics/top-picks.xml</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid    Type                                              Title  \\\n",
       "0  0.0  source                                                @IT   \n",
       "1  NaN    None  https://news.yahoo.co.jp/rss/topics/top-picks.xml   \n",
       "\n",
       "                                                 URL  \\\n",
       "0                    https://atmarkit.itmedia.co.jp/   \n",
       "1  https://news.yahoo.co.jp/rss/topics/top-picks.xml   \n",
       "\n",
       "                                                 URI Description  \n",
       "0                    https://atmarkit.itmedia.co.jp/   Atmark IT  \n",
       "1  https://news.yahoo.co.jp/rss/topics/top-picks.xml              "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML or rdf\n"
     ]
    }
   ],
   "source": [
    "from xml.etree import ElementTree\n",
    "import streamlit  as st\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extract_rss_urls(website_url):\n",
    "    response = requests.get(website_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    rss_urls = []\n",
    "    for link in soup.find_all('link', type='application/rss+xml'):\n",
    "        rss_urls.append(link.get('href'))\n",
    "\n",
    "    return rss_urls\n",
    "\n",
    "def is_valid_url(url):\n",
    "    # Regular expression to validate an HTML URL\n",
    "    url_regex = re.compile(\n",
    "        r'^(https?|ftp)://'  # http://, https://, ftp://\n",
    "        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|'  # domain...\n",
    "        r'localhost|'  # localhost...\n",
    "        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}|'  # ...or ipv4\n",
    "        r'\\[?[A-F0-9]*:[A-F0-9:]+\\]?)'  # ...or ipv6\n",
    "        r'(?::\\d+)?'  # optional port\n",
    "        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "    \n",
    "    # Check if the URL matches the pattern\n",
    "    return re.match(url_regex, url) is not None\n",
    "\n",
    "def is_xml_extension(url):\n",
    "    # Check if the URL ends with '.xml'\n",
    "    return url.lower().endswith('.xml')\n",
    "\n",
    "def is_xml_or_rdf(url):\n",
    "    # Check if the URL ends with .xml or .rdf\n",
    "    if url.lower().endswith(('.xml', '.rdf')):\n",
    "        return True\n",
    "\n",
    "# new_site_url = \"https://win-tab.net/feed/\"\n",
    "# new_site_url = \"https://pc.watch.impress.co.jp/\"\n",
    "new_site_url='https://pc.watch.impress.co.jp/data/rss/1.0/pcw/feed.rdf'\n",
    "new_site_label = \"PC\"\n",
    "\n",
    "# if is_xml_extension(new_site_url):\n",
    "#     print(\"XML\")\n",
    "# else:\n",
    "#     print(\"Not XML\")\n",
    "        \n",
    "if is_valid_url(new_site_url):\n",
    "    if is_xml_or_rdf(new_site_url):\n",
    "        print(\"XML or rdf\")\n",
    "        rss_feeds = []\n",
    "        # rss_feeds += [new_site_url, new_site_label]\n",
    "        rss_feeds.append('new_site_url')\n",
    "    else:\n",
    "        print(\"not XML or rdf\")\n",
    "        rss_feeds = extract_rss_urls(new_site_url)\n",
    "                \n",
    "        # rss_feeds = extract_rss_urls(new_site_url)\n",
    "    \n",
    "    for item in rss_feeds:\n",
    "        feed = feedparser.parse(item)\n",
    "        for entry in feed.entries:\n",
    "            if 'published' in entry:\n",
    "                print(entry.published)\n",
    "            print(entry.title)\n",
    "            print(entry.link)\n",
    "            print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "feeds = extract_rss_urls(\"https://pc.watch.impress.co.jp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://pc.watch.impress.co.jp/data/rss/1.0/pcw/feed.rdf']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_rss_urls(\"https://pc.watch.impress.co.jp/data/rss/1.0/pcw/feed.rdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
