{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class rsss_feed:\n",
    "    def __init__(self, source_site_name, title, url, uri, created_date):\n",
    "        self.source_site_name = source_site_name\n",
    "        self.title = title\n",
    "        self.url = url\n",
    "        self.uri = uri\n",
    "        self.created_date = created_date\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"rsss_feed(title={self.title}, url={self.url})\"\n",
    "\n",
    "class source_site:\n",
    "    def __init__(self, title, url):\n",
    "        self.title = title\n",
    "        self.url = url\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"source_site(titel={self.title}, url={self.url})\"\n",
    "    \n",
    "    def add_rss_feed(self, rss_feed):\n",
    "        self.children.append(rss_feed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = source_site(\"source\", \"@IT\", \"https://atmarkit.itmedia.co.jp/\", \"https://atmarkit.itmedia.co.jp/\", \"Atmark IT\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "initial_data = [[0,\"source\", \"@IT\", \"\\\"https://atmarkit.itmedia.co.jp/\\\"\", \"\\\"https://atmarkit.itmedia.co.jp/\\\"\", \"Atmark IT\"]]\n",
    "\n",
    "df = pd.DataFrame(initial_data, columns=[\"uid\", \"Type\", \"Title\", \"URL\", \"URI\", \"Description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringA = df[df['uid'] == 0]['URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[3,2,1,0]]\n",
    "\n",
    "df1 = pd.DataFrame(data1, columns=[\"A\", \"B\", \"C\", \"D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strA = df1[df1['A'] == 3]['B']\n",
    "print (strA.astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strB = df1.query('A == 3')['B']\n",
    "elementB = df1.loc[0, 'B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elementB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# initial_data = [[0,\"source\", \"@IT\", \"\\\"https://atmarkit.itmedia.co.jp/\\\"\", \"\\\"https://atmarkit.itmedia.co.jp/\\\"\", \"Atmark IT\"]]\n",
    "initial_data = [[0,\"source\", \"@IT\", \"https://atmarkit.itmedia.co.jp/\", \"https://atmarkit.itmedia.co.jp/\", \"Atmark IT\"]]\n",
    "  \n",
    "    \n",
    "sites_df = pd.DataFrame(initial_data, columns=[\"uid\", \"Type\", \"Title\", \"URL\", \"URI\", \"Description\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_site_url=\"https://akiba-pc.watch.impress.co.jp/\"\n",
    "new_row = pd.DataFrame({'URL': [new_site_url], 'URI': [new_site_url]})\n",
    "sites_df= pd.concat([sites_df, new_row], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def sample_df():\n",
    "    initial_data = [[0,\"source\", \"@IT\", \"https://atmarkit.itmedia.co.jp/\", \"https://atmarkit.itmedia.co.jp/\", \"Atmark IT\"]]\n",
    "    sample_df = pd.DataFrame(initial_data, columns=[\"uid\", \"Type\", \"Title\", \"URL\", \"URI\", \"Description\"])\n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"default.json\"\n",
    "if not os.path.exists(filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        sites_df.to_json(filename, orient='records')\n",
    "else:\n",
    "    sites_df = pd.read_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Date string\n",
    "date_str = \"Thu, 25 Jul 2024 10:00:00 +0900\"\n",
    "\n",
    "# Parse the date string\n",
    "date_object = datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S %z')\n",
    "\n",
    "# Print the resulting datetime object\n",
    "print(date_object.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "# String variables\n",
    "text = \"Hello, World!\"\n",
    "color = \"blue\"\n",
    "font_size = \"24px\"\n",
    "\n",
    "# Create the HTML string with the variables\n",
    "html_str = f\"<p style='font-size:{font_size}; color:{color};'>{text}</p>\"\n",
    "\n",
    "# Display the styled text in Streamlit\n",
    "st.markdown(html_str, unsafe_allow_html=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "website_url = \"https://atmarkit.itmedia.co.jp/\"\n",
    "\n",
    "response = requests.get(website_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "rss_urls = []\n",
    "for link in soup.find_all('link', type='application/rss+xml'):\n",
    "    rss_urls.append(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "url = \"https://news.yahoo.co.jp/rss/topics/top-picks.xml\"\n",
    "\n",
    "feed = feedparser.parse(url)\n",
    "for entry in feed.entries:\n",
    "    if 'published' in entry:\n",
    "        print(entry.published)\n",
    "    print(entry.title)\n",
    "    print(entry.link)\n",
    "    #print(entry.summary)\n",
    "    print('---------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from xml.etree import ElementTree\n",
    "import streamlit  as st\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "filename_news= \"newsfeed.json\"\n",
    "news_df = pd.read_json(filename_news)\n",
    "\n",
    "print(news_df)\n",
    "\n",
    "rss_feeds = [news_df.loc[1, 'URL']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in rss_feeds:\n",
    "    feed = feedparser.parse(item)\n",
    "    # st.text(rss_feeds)\n",
    "    # st.text(len(feed))\n",
    "    # st.text(feed)\n",
    "    for entry in feed.entries:\n",
    "        if 'published' in entry:\n",
    "            print(entry.published)\n",
    "        print(entry.title)\n",
    "        print(entry.link)\n",
    "        print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree\n",
    "import streamlit  as st\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "filename_news= \"newsfeed.json\"\n",
    "news_df = pd.read_json(filename_news)\n",
    "\n",
    "print(\"Add news rss from site\")\n",
    "\n",
    "print(news_df)\n",
    "# print(news_df.query('Type == source')['url'])\n",
    "print(news_df[news_df['Type'] == 'source']['URL'])\n",
    "\n",
    "print(\"Add news rss\")\n",
    "print(news_df[news_df['Type'] != 'source'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df\n",
    "df=news_df\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "        # Create a new row with columns\n",
    "        row_as_list = row.to_list()\n",
    "\n",
    "        # Display the item in the first column\n",
    "        print(row_as_list[1],row_as_list[2])\n",
    "        print(index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_as_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree\n",
    "import streamlit  as st\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extract_rss_urls(website_url):\n",
    "    response = requests.get(website_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    rss_urls = []\n",
    "    for link in soup.find_all('link', type='application/rss+xml'):\n",
    "        rss_urls.append(link.get('href'))\n",
    "\n",
    "    return rss_urls\n",
    "\n",
    "def is_valid_url(url):\n",
    "    # Regular expression to validate an HTML URL\n",
    "    url_regex = re.compile(\n",
    "        r'^(https?|ftp)://'  # http://, https://, ftp://\n",
    "        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|'  # domain...\n",
    "        r'localhost|'  # localhost...\n",
    "        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}|'  # ...or ipv4\n",
    "        r'\\[?[A-F0-9]*:[A-F0-9:]+\\]?)'  # ...or ipv6\n",
    "        r'(?::\\d+)?'  # optional port\n",
    "        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "    \n",
    "    # Check if the URL matches the pattern\n",
    "    return re.match(url_regex, url) is not None\n",
    "\n",
    "def is_xml_extension(url):\n",
    "    # Check if the URL ends with '.xml'\n",
    "    return url.lower().endswith('.xml')\n",
    "\n",
    "def is_xml_or_rdf(url):\n",
    "    # Check if the URL ends with .xml or .rdf\n",
    "    if url.lower().endswith(('.xml', '.rdf')):\n",
    "        return True\n",
    "\n",
    "# new_site_url = \"https://win-tab.net/feed/\"\n",
    "# new_site_url = \"https://pc.watch.impress.co.jp/\"\n",
    "# new_site_url='https://pc.watch.impress.co.jp/data/rss/1.0/pcw/feed.rdf'\n",
    "new_site_url=\"https://pc.watch.impress.co.jp/data/rss/1.0/pcw/feed.rdf\"\n",
    "new_site_label = \"PC\"\n",
    "\n",
    "# if is_xml_extension(new_site_url):\n",
    "#     print(\"XML\")\n",
    "# else:\n",
    "#     print(\"Not XML\")\n",
    "        \n",
    "if is_valid_url(new_site_url):\n",
    "    if is_xml_or_rdf(new_site_url):\n",
    "        print(\"XML or rdf\")\n",
    "        rss_feeds = []\n",
    "        # rss_feeds += [new_site_url, new_site_label]\n",
    "        rss_feeds.append('new_site_url')\n",
    "    else:\n",
    "        print(\"not XML or rdf\")\n",
    "        rss_feeds = extract_rss_urls(new_site_url)\n",
    "                \n",
    "        # rss_feeds = extract_rss_urls(new_site_url)\n",
    "    \n",
    "    for item in rss_feeds:\n",
    "        feed = feedparser.parse(item)\n",
    "        for entry in feed.entries:\n",
    "            if 'published' in entry:\n",
    "                print(entry.published)\n",
    "            print(entry.title)\n",
    "            print(entry.link)\n",
    "            print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feeds = extract_rss_urls(\"https://pc.watch.impress.co.jp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_rss_urls(\"https://pc.watch.impress.co.jp/data/rss/1.0/pcw/feed.rdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from rdflib import Graph\n",
    "\n",
    "# website_url = \"https://win-tab.net\"\n",
    "# website_url = \"https://atmarkit.itmedia.co.jp/\"\n",
    "# website_url = \"https://win-tab.net/feed/\"\n",
    "# website_url = \"https://pc.watch.impress.co.jp/\"\n",
    "website_url ='https://pc.watch.impress.co.jp/data/rss/1.0/pcw/feed.rdf'\n",
    "# website_url =\"https://pc.watch.impress.co.jp/data/rss/1.0/pcw/feed.rdf\"\n",
    "\n",
    "response = requests.get(website_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "rss_urls = []\n",
    "\n",
    "if website_url.lower().endswith(('.rdf')):\n",
    "    # Initialize the graph\n",
    "    g = Graph()\n",
    "    # Parse the RDF data from a URL or a file\n",
    "    g.parse(\"http://example.org/some-rdf-data\")\n",
    "    # Define the namespace for Dublin Core, which often contains the title\n",
    "    from rdflib.namespace import DC\n",
    "    # Query the graph for the title\n",
    "    title = None\n",
    "    for s, p, o in g:\n",
    "        if p == DC.title:\n",
    "            title = o\n",
    "            break\n",
    "\n",
    "    # Print the title\n",
    "    if title:\n",
    "        print(\"Title:\", title)\n",
    "    else:\n",
    "        print(\"Title not found.\")\n",
    "else:\n",
    "    for link in soup.find_all('link', type='application/rss+xml'):\n",
    "        rss_urls.append([link.get('title'), link.get('href')])\n",
    "        print(link.get('title'), link.get('href'))\n",
    "\n",
    "print(rss_urls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "# Initialize the graph\n",
    "g = Graph()\n",
    "\n",
    "# Parse the RDF data from a URL or a file\n",
    "g.parse(\"http://example.org/some-rdf-data\")\n",
    "# g.parse(\"https://pc.watch.impress.co.jp/data/rss/1.0/pcw/feed.rdf\")\n",
    "\n",
    "# Define the namespace for Dublin Core, which often contains the title\n",
    "from rdflib.namespace import DC\n",
    "\n",
    "# Query the graph for the title\n",
    "title = None\n",
    "for s, p, o in g:\n",
    "    if p == DC.title:\n",
    "        title = o\n",
    "        break\n",
    "\n",
    "# Print the title\n",
    "if title:\n",
    "    print(\"Title:\", title)\n",
    "else:\n",
    "    print(\"Title not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef\n",
    "from rdflib.namespace import DC, DCTERMS, FOAF\n",
    "\n",
    "# Initialize the graph\n",
    "g = Graph()\n",
    "\n",
    "# Parse the RDF data from a URL\n",
    "url = \"https://pc.watch.impress.co.jp/data/rss/1.0/pcw/feed.rdf\"\n",
    "g.parse(url, format=\"xml\")\n",
    "\n",
    "# Attempt to find the title using different common namespaces\n",
    "title_predicates = [\n",
    "    DC.title,         # Dublin Core title\n",
    "    DCTERMS.title,    # Dublin Core Terms title\n",
    "    FOAF.name,        # FOAF name (sometimes used as a title)\n",
    "]\n",
    "\n",
    "title = None\n",
    "\n",
    "# Search for the title in the RDF graph\n",
    "for predicate in title_predicates:\n",
    "    for s, p, o in g.triples((None, predicate, None)):\n",
    "        title = o\n",
    "        break\n",
    "    if title:\n",
    "        break\n",
    "\n",
    "# Print the extracted title\n",
    "if title:\n",
    "    print(\"Title:\", title)\n",
    "else:\n",
    "    print(\"Title not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 22:44:45.191 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\chien\\OneDrive\\_PARA\\1.Projects\\RssProj\\myenv\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-08-20 22:44:45.192 Session state does not function when running a script without `streamlit run`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main.py\n",
    "from xml.etree import ElementTree\n",
    "import streamlit  as st\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extract_urls_title(website_url):\n",
    "    response = requests.get(website_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    rss_urls = []\n",
    "    for link in soup.find_all('link', type='application/rss+xml'):\n",
    "        rss_urls.append([link.get('title'), link.get('href')])\n",
    "\n",
    "    return rss_urls\n",
    "\n",
    "def is_valid_url(url):\n",
    "    # Regular expression to validate an HTML URL\n",
    "    url_regex = re.compile(\n",
    "        r'^(https?|ftp)://'  # http://, https://, ftp://\n",
    "        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|'  # domain...\n",
    "        r'localhost|'  # localhost...\n",
    "        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}|'  # ...or ipv4\n",
    "        r'\\[?[A-F0-9]*:[A-F0-9:]+\\]?)'  # ...or ipv6\n",
    "        r'(?::\\d+)?'  # optional port\n",
    "        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "    \n",
    "    # Check if the URL matches the pattern\n",
    "    return re.match(url_regex, url) is not None\n",
    "\n",
    "def is_xml_extension(url):\n",
    "    # Check if the URL ends with '.xml'\n",
    "    return url.lower().endswith('.xml')\n",
    "\n",
    "def is_xml_or_rdf(url):\n",
    "    # Check if the URL ends with .xml or .rdf\n",
    "    if url.lower().endswith(('.xml', '.rdf')):\n",
    "        return True\n",
    "def is_rdf_extension(url):\n",
    "    # Check if the URL ends with '.xml'\n",
    "    return url.lower().endswith('.rdf')\n",
    "\n",
    "def sample_df():\n",
    "    initial_data = [[0,\"rss\", \"@IT\", \"https://rss.itmedia.co.jp/rss/1.0/ait.xml\", \"https://rss.itmedia.co.jp/rss/1.0/ait.xml\", \"Atmark IT\",\"Tech\"]]\n",
    "    sample_df = pd.DataFrame(initial_data, columns=[\"uid\", \"Type\", \"Title\", \"URL\", \"URI\", \"Description\",\"label\"])\n",
    "    return sample_df \n",
    "\n",
    "\n",
    "st.title(\"Add new sits or rss\")\n",
    "\n",
    "\n",
    "df = sample_df()\n",
    "\n",
    "st.text(df)\n",
    "message = \"site or rss url\"\n",
    "\n",
    "# Add a new row\n",
    "st.write(\"Add a new site:\")\n",
    "new_site_url = st.text_input(message)\n",
    "new_site_label = st.text_input(\"site label\")\n",
    "\n",
    "if st.button(\"Add site\"):\n",
    "    rss_feeds = []\n",
    "    if is_valid_url(new_site_url):\n",
    "        st.text(new_site_url)\n",
    "        if is_rdf_extension(new_site_url):\n",
    "            rss_feeds.append('URL')\n",
    "        else:\n",
    "            rss_feeds = extract_urls_title(new_site_url)\n",
    "            \n",
    "        for row in rss_feeds:\n",
    "            col1, col2 = st.columns([3, 1])  # Adjust the proportions as needed\n",
    "\n",
    "            with col1:\n",
    "                st.write(row[0],row[1])\n",
    "\n",
    "            # Display the first action button in the second column\n",
    "            with col2:\n",
    "                if st.button(\"add\", key=f\"add_key{row}\"):\n",
    "                    st.text(f\"Added add_key{row}\")\n",
    "                    #to do add to df\n",
    "                    print(new_site_label,new_site_url)\n",
    "                    new_row = pd.DataFrame({'label': [row[0]],'URL': [row[1]], 'URI': [row[1]]})\n",
    "                    st.dataframe(new_row)\n",
    "                    st.session_state.df  = pd.concat([st.session_state.df , new_row], ignore_index=True)\n",
    "                    st.text(st.session_state.df )\n",
    "                    print (st.session_state.df )\n",
    "\n",
    "\n",
    "    else:\n",
    "        message = \"Enter site or rss url again\"\n",
    "\n",
    "st.dataframe(st.session_state.df)\n",
    "\n",
    "    # st.write(\"Updated DataFrame:\")\n",
    "    # st.dataframe(df)\n",
    "    # if st.button('Refresh Data'):\n",
    "    #     st.dataframe(df)\n",
    "    # Your data fetching logic here\n",
    "        # with open(\"default.json\", 'w') as file:\n",
    "        #     df.to_json(\"default.json\", orient='records')\n",
    "        # st.write('Data refreshed!')\n",
    "        \n",
    "    # return df\n",
    "   \n",
    "    \n",
    "# df = manage_sites(st.session_state[\"default_df\"])\n",
    "# st.session_state[\"default_df\"]=df\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3870539599.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    streamlit run c:\\Users\\chien\\OneDrive\\_PARA\\1.Projects\\RssProj\\myenv\\Lib\\site-packages\\ipykernel_launcher.py\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "streamlit run c:\\Users\\chien\\OneDrive\\_PARA\\1.Projects\\RssProj\\myenv\\Lib\\site-packages\\ipykernel_launcher.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
